---
slug: web-crawling-ii
title: Distributed Web Crawling - II Distributing Things 
date: 2021-03-01
author: Dan
tags:
  - web-crawling
  - python
  - messaging
  
---
import Messaging from "../../assets/messaging.svg";
import MessagingV2 from "../../assets/messaging-scale.svg";
import MessagingExample from "../../assets/messaging-example.svg";

The first thing I think of when I need to distribute a service is message queues, the second thing I think of is a cache. The queues so that
the distributed nodes can talk to each other with resilience built in, and the cache so that they can have some shared memory.


## First things first

Planning. What services do I need? How should they communicate? In my last post I layed out the core components of
a web crawler
>
- The 'Engine' which will retrieve page contents and distribute it to the other components
- Result outputters which will process the content and output where necessary
- Link extractors which will extract further links from the contents  
>

Each of these will be a service type.


As for how they communicate I mentioned earlier we would use message queues how? We know that the engine needs to talk to the other 2 
services, since it needs to send them the page content and it also needs to listen for links generated by the link-extractor. The other 2 components
need for the output of the 'Engine' and the link-extractor needs to output, you guessed it, links. 

<figure className="chart">
  <Messaging></Messaging>
  <figcaption>Giving us a structure like this</figcaption>
</figure>

## Building for Scale


### Messaging

This gets us started and will work well with one of each service. **But** we're making this distributed so that we can scale each individual component, our messaging needs to support this too. Suppose we 
have two types of Link Extractor, two types of Result Outputter, our Engine and we want 2 instances of each. The above diagram quickly becomes a mess. We need the messages to be shared between and delivered only once to each of the individual types of component 
regardless of how many instances we have of each. Messages sent back to the engine must be delivered to only one instance of it. 

How do we begin to structure this? Enter RabbitMq, most message brokers support these types of messaging but for the purposes of this blog I will talk Using
RabbitMq specific terminology.

With RabbitMq we have the ability message in different ways, the two we want are direct and topic based messaging, you can read more about this here https://www.rabbitmq.com/tutorials/amqp-concepts.html.

**Direct messaging** is straight forward, you have a queue and a bunch of services can listen it and take a message when there is one to take, easy. 

**Topic based messaging** is a little more interesting, each queue connects to an exchange and tells it what type of message it wants to receive,
this is called the routing key. When a message matching the definition of the queue is delivered to the exchange, it is then sent to  **all** of the queues listening for it. Taking this into account we can restructure our messaging 
diagram to look like this.

<figure className="chart">
  <MessagingV2></MessagingV2>
</figure>

Now we have the Engine delivering messages to the 'result' exhange. The routing key of the messages can be our current URL allowing services to take only messages from certain URLs. 
RabbitMq gives us the ability to use wildcards in the routing key. For example, if we have the URL `www.abc.com/some/path` if we were to replace the
`/'s` with `.'s`, giving us `www.abc.com.some.path` this would be a great routing key. 

This key would match queues defined with routing keys of:
- `www.abc.com.some.path`
- `www.abc.com.#`
- or even just `#`

This is pretty powerful as it gives us the ability to define Link-Extractors or Result-Outputters that are relevant to specific areas of a website or multiple websites.


### Caching

One thing that we need to make sure of is that we only visit each link once throughout the crawl, otherwise we could end up in an infinite loop. In a crawl performed by a single process we could
simply keep a list of the URLs as we visit them and check it before visiting another. Now that we are distributed we need a shared place to store this information. A Redis set is the simplest
solution for this as it allows us to store a list of strings available to multiple processes.


## A concrete example 

As in my previous post, lets continue to use https://news.ycombinator.com/news as our example site, and we want to print the title of each of the articles that are linked. How would we set up our
services? 
Taking a look at the page you can see that there is a list of articles and a 'more' link to the next page of atricles. Each of these are links that we are going to want to follow so we have a case for 2 
link extractors that will listen to any page that starts with news.ycombinator.com/news. We want to print the title of each article page we visit, this is the job of our result outputter, given that
each article is on a different site we will need this to listen to messages from any url.

<figure className="chart">
  <MessagingExample></MessagingExample>
</figure>

## The Code

The framework I have written allowing all of this to take place can be found at https://github.com/DanielLyons178/clustered-crawler. This consists of the core engine, and extensible classes for
the link-extraction and result-outputters, meaning users just need to implement functions to extract the data they want and run them along with the core engine, with as many instances of each as they desire.

### Client Code

Referring to the example above the following would be an implementation of a a link extractor to retrieve the article links on https://news.ycombinator.com/news.

```python
from bs4 import BeautifulSoup
from bs4.element import SoupStrainer
from scraper.client.lib.link_extraction.link_extractor import LinkExtractor


class SampleExtractor(LinkExtractor):
    def extract_links(self, result):
        soup = BeautifulSoup(result['res'], parse_only=SoupStrainer("a"), features="html.parser")
        return [
            link["href"]
            for link in soup.find_all(attrs={"class": "storylink"})
            if link.has_attr("href")
        ]

    def for_links_pattern(self):
        return ["news.ycombinator.com.#"]
```

__for_links_pattern__ defines the pages that this extractor should run against (note the __#__ wildcard) and __extract_links__ grabs the links from the page html. Straight forward right? 

And the result outputter.


```python
from bs4 import BeautifulSoup
from scraper.client.lib.body_reader.body_reader import BodyReader


class SampleReader(BodyReader):

    def process_body(self, html):
        soup = BeautifulSoup(html, features="html.parser")
        title = soup.title.string if soup.title else ''
        print(title)        

    def for_links_pattern(self):
        return ["#"]
```

__for_links_pattern__ does the same job as for the LinkExtractor and the __process_body__ receives the html for you to do as you choose with, in this case print the title.

These are the only two bits of code for the client to write to begin scraping.

### The Engine

The engine code is also fairly simple, given implementations of a link reciever, result outputters (RabbitMq queues in this case) and a visit cacher (Redis in this case), it is defined as the following.


```python
from .visitor.visitor import Visitor
import logging


class ScraperEngine:
    def __init__(
        self, link_reciever, visit_cacher, result_outputters=[]
    ) -> None:
        self.visitor = Visitor()
        self.link_reciever = link_reciever
        self.result_outputters = result_outputters
        self.visit_cacher = visit_cacher
        self.logger = logging.getLogger("Engine")

    def run(self):
        self.logger.info("Listening for links")
        self.link_reciever.poll(self.process_link)

    def process_link(self, link):
        self.logger.info("Processing link %s", link)
        if not self.visit_cacher.is_visited(link):
            result = self.visitor.get(link)
            for outputter in self.result_outputters:
                outputter.put((link, result))
            self.visit_cacher.put(link)
```

Note that that it listens for links using the __process_link__ method as a callback, then checks the cache to see if the link has already been visited and if not proceeds to visit and send the output to all of the relevant
Link-Extractors and Result-Outputters.

Here are the abstract classes defined for the Link-Extractor and Result-Outputter:

```python
from abc import ABC, abstractmethod


class BodyReader(ABC):
    def __init__(self) -> None:
        super().__init__()

    def run(self):
        self._receiver.poll(self.process_body)

    def register_receiver(self, rec):
        self._receiver = rec

    @abstractmethod
    def process_body(self, html):
        return None

    @abstractmethod 
    def for_links_pattern(self):
        return ['#']
```

```python
from abc import ABC, abstractmethod
import json


class LinkExtractor(ABC):
    def __init__(self, sender) -> None:
        super().__init__()
        self._sender = sender

    def process_links(self, html):
        links = self.extract_links(json.loads(html))
        for link in links:
            self._sender.put(link)

    def run(self):
        self._receiver.poll(self.process_links)

    def register_receiver(self, rec):
        self._receiver = rec

    @abstractmethod
    def extract_links(self, html):
        return []

    @abstractmethod
    def for_links_pattern(self):
        return ''
```

These abstract classes handle the communication between the services so all the client needs to worry about is how they want to deal with the data they receive.


## Next

Next I will focus on making it as straight forawrd as possible to start everything up and kick off a crawl. Then we can see it in action.